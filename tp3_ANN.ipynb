{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import History,LearningRateScheduler\n",
    "from  tensorflow.keras.layers import Dropout\n",
    "print('TensorFlow %s, Keras %s, numpy %s, pandas %s'%(tf.__version__,keras.__version__, np.__version__,pd.__version__))\n",
    "__DEBUG__=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Structure du réseau et nombre d'epochs (nombre de fois où on passe sur le DataSet)\n",
    "num_hidden_layers=4\n",
    "first_layer_size = 128\n",
    "other_layer_size = 512\n",
    "epochs=50\n",
    "\n",
    "###Valeurs A tester dans la cross validation\n",
    "lst_init_learning_rate = [0.01,0.003, 0.1] \n",
    "lst_dropout_prob=[0.15,0.05]\n",
    "n_splits=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcule les valeurs min/max et moyennes de chaque colonne dans lst_cols du dataframe pandas  df\n",
    "def get_columns_metadata(df, lst_cols):            \n",
    "     header_df = pd.DataFrame( data = lst_cols, columns=['var_name'])    \n",
    "     header_df['mean']=df[lst_cols].mean().values\n",
    "     header_df['min']= df[lst_cols].min().values\n",
    "     header_df['max']= df[lst_cols].max().values\n",
    "     header_df.set_index('var_name',inplace=True)\n",
    "     return header_df\n",
    "\n",
    "#Normalisation de chaque colonne du dataframe pandas  df en utilisant les valeurs de header_df\n",
    "def normalize(df,header_df):\n",
    "    for col in df.columns:        \n",
    "        if col in header_df.index : \n",
    "### Ici normùaliser chaque  colonne. Pour l'instant on ne fait rien\n",
    "            df[col] = df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25b1e1db-8bc5-7029-f719-91da523bd121",
    "_uuid": "5c867fcbb300bcf3c9b8986bba9949da2a2df931"
   },
   "source": [
    "## lecture des données ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2ce68358-02ec-556d-ba88-e773a50bc18b",
    "_uuid": "5ec0878acc5c7ab3903410e671c2a2c6cfeafeea"
   },
   "outputs": [],
   "source": [
    "# La fonction pandas pd.read_csv permet de créer un objet Dataframe à partir d'un csv\n",
    "\n",
    "# Données avec labels\n",
    "train = pd.read_csv('Data/passagers.csv', header = 0, dtype={'Age': np.float64})\n",
    "# Données de tests sans label. Les prédictions de survie seront envoyées à kaggle\n",
    "test  = pd.read_csv('Data/test.csv' , header = 0, dtype={'Age': np.float64})\n",
    "# On réunit les données dans une liste (pour pouvoir boucler sur les 2 dataframes)\n",
    "full_data = [train, test]\n",
    "#On garde les passagers ID des données test, car on en aura besoin pour le fichiers résultats de kaggle (voir l'exemple gender_submission.csv)\n",
    "finalfile_index=test.PassengerId #Index des données de test pour le résultat final\n",
    "\n",
    "#La fonction info() permet de répérer les colonnes avec des valeurs nulles\n",
    "print('\\nTrain data:')\n",
    "train.info()\n",
    "print('\\nTest data:')\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f9595646-65c9-6fc4-395f-0befc4d122ce",
    "_uuid": "66273d64a2548d7a88464ab2a73dbdedfbdc488b"
   },
   "source": [
    "# Analyse des données #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9b4c278b-aaca-e92c-ba77-b9b48379d1f1",
    "_uuid": "c2b62e14d493c270ec8df9f3af1938c479361ef3"
   },
   "source": [
    "## 1. Pclass ##\n",
    "Impact de la classe sur la Survie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4680d950-cf7d-a6ae-e813-535e2247d88e",
    "_uuid": "f02533e7b85bba0cca7fcf2cc598c8da92d7646d"
   },
   "outputs": [],
   "source": [
    "print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e70f81c-d4e2-1823-f0ba-a7c9b46984ff",
    "_uuid": "3ca2394409e52b8d6c40d13b6ce557c85e4cd4fd"
   },
   "source": [
    "## 2. Sex ##\n",
    "Impact du genre sur la Survie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6729681d-7915-1631-78d2-ddf3c35a424c",
    "_uuid": "2b50b53008fa018127b9d9ee2fb519347b22edcc"
   },
   "outputs": [],
   "source": [
    "print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c58b7ee-d6a1-0cc9-2346-81c47846a54a",
    "_uuid": "88185e9222c26d5d23caaeb209c18710a231b5f9"
   },
   "source": [
    "## 3. SibSp and Parch ##\n",
    "Impacte de la taille de la famille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a537f10-7cec-d0b7-8a34-fa9975655190",
    "_uuid": "ccc4a4cf7624dd4be450fd62ca3ad478d4e75696"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e4861d3e-10db-1a23-8728-44e4d5251844",
    "_uuid": "f8d7354e5c9160a7da108726a752f7dc366cb0aa"
   },
   "source": [
    "Introduction d'une distinction sur les personnes seules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c35e945-c928-e3bc-bd9c-d6ddb287e4c9",
    "_uuid": "87f79dc0711c29f39c0db1a4f7a2e8a84c0c7edb"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8aa419c0-6614-7efc-7797-97f4a5158b19",
    "_uuid": "dd18a31086cfeca6330b05f83caf3cc02f687253"
   },
   "source": [
    "## 4. Embarked ##\n",
    "Impact du Port d'embarquement sur la Survie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e70e9af-d7cc-8c40-b7d4-2643889c376d",
    "_uuid": "b4f7ccb3df98da6915bda1c7c225b905fc37845b"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e08c9ee8-d6d1-99b7-38bd-f0042c18a5d9",
    "_uuid": "bbeb369d4bb1b086fcc3257218fdeeb6bcdb53c0"
   },
   "source": [
    "## 5. Fare ##\n",
    "On remplace les valeurs manquantes par la moyenne. Puis on regarde l'impact du prix du ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset.loc[dataset.Fare.isnull(), 'Fare'] = train['Fare'].mean()\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec8d1b22-a95f-9f16-77ab-7b60d2103852",
    "_uuid": "7d96d8817432fa25d8acbcb229df0bd0633b75fa"
   },
   "source": [
    "## 6. Age ##\n",
    "Pour les valeurs vides, on gnère des ages aléatoires entre (mean - std) and (mean + std).\n",
    "Ensuite on analyse l'impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b90c2870-ce5d-ae0e-a33d-59e35445500e",
    "_uuid": "2af2b56d51752be08b84dbb2684466976758faa7"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean() # Calcul de la valeur moyenne\n",
    "    age_std = dataset['Age'].std()  # Calcul de l'écart type\n",
    "    age_null_count = dataset['Age'].isnull().sum() # nombre de valuer nulle\n",
    "    \n",
    "    #On génère une valeur aléatoire pour chaque valeur nulle, puis on l'arrondit à l'entier\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)   \n",
    "    dataset.loc[np.isnan(dataset['Age']),'Age'] = age_null_random_list    \n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "#Impact de l'age sur le taux de survie\n",
    "train['CategoricalAge'] = pd.qcut(train['Age'],5)\n",
    "print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68fa2057-e27a-e252-0d1b-869c00a303ba",
    "_uuid": "52bcf7b36b8edb12d40f2a1f9e80060b55d91ad3"
   },
   "source": [
    "# Mise en Forme des données #\n",
    "### > Remplacement des données textuelles par des données numériques\n",
    "### > Suppressions des colonnes inutiles (sans impact sur la survie ou créées ci-dessus)###\n",
    "\n",
    "## ATTENTION : Il faut lancer \"Run All Above Selected Cell\" dans le menu Run pour pouvoir relancer ce bloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2502bb70-ce6f-2497-7331-7d1f80521470",
    "_uuid": "1aa110c1043f1f43c091a771abc64054a211f784"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    # Traitement variable 'Sex'\n",
    "    dataset['Sex'].replace('female',0,inplace=True )\n",
    "    dataset['Sex'].replace('male',1,inplace=True)\n",
    "    \n",
    "   # Traitement variable 'Embarked'\n",
    "    dataset['Embarked'].replace('S',0,inplace=True)\n",
    "    dataset['Embarked'].replace('C',1,inplace=True)\n",
    "    dataset['Embarked'].replace('Q',2,inplace=True)    \n",
    "\n",
    "# Suppression des colonnes inutiles (Traitements différents sur Train et Test => on ne peut pas mettre ces instruction dans la boucle)\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp','Parch', 'FamilySize']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "### N'oubliez pas de mettre à jour la fonction normalize !\n",
    "header_df=get_columns_metadata(train,list(train.columns.values)) \n",
    "print(header_df)\n",
    "normalize(train,header_df)\n",
    "\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "normalize(test,header_df)\n",
    "\n",
    "print('\\nTrain data:')\n",
    "print (train.head(10))\n",
    "print('\\nTest data:')\n",
    "print (test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle et initialisation Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(init_learning_rate,dropout_prob):\n",
    "    #Architecture du réseau\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(first_layer_size, activation='relu'))\n",
    "    \n",
    "### Ajouter ici une ligne  pour gérer le sur-apprentissage\n",
    "\n",
    "    #Couches cachées (Hidden Layers)\n",
    "    for i in range(num_hidden_layers):\n",
    "        # Adds a densely-connected layer  to the model:\n",
    "        model.add(keras.layers.Dense(other_layer_size, activation='relu'))\n",
    "### Ajouter ici une ligne  pour gérer le sur-apprentissage\n",
    "    # Couche de Sortie (avec fonction Softmax):\n",
    "    model.add(keras.layers.Dense(2, activation='softmax'))    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step,1000, 0.96, staircase=True)\n",
    "    \n",
    "\n",
    "### Ici vous pouvez essayer différents algos de descentes de gradients \n",
    "    #Définiton de l'optimizer  en charge de la Gradient Descent, de la fonction de coût et de la métrique.\n",
    "    model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate),#RMSPropOptimizer(learning_rate), #GradientDescentOptimizer(learning_rate),AdamOptimizer\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification du Sur-Apprentissage ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Essayez Différents jeu de paramètre pour réduire le sur-appentissage\n",
    "init_learning_rate=0.1\n",
    "dropout_prob= 0\n",
    "check_epochs=200\n",
    "pourcentage_validation= 0.2\n",
    "\n",
    "#A partir des données Train, on sépare features (X)  et labels \"Survived\"\n",
    "lst_col=list(train.columns.values)\n",
    "lst_col.remove('Survived')\n",
    "X=train[lst_col]\n",
    "y=train['Survived']\n",
    "\n",
    "# On calcule la position de la séparation pour une répartition 80/20\n",
    "position_validation_data=int(train.shape[0] * (1-pourcentage_validation))\n",
    "print('position_validation_data=',position_validation_data)\n",
    "\n",
    "# Construction des Features pour l'apprentissage et la validation.  Transformation du Dataframe Pandas en Numpy Array (attendu par Keras) \n",
    "X_train, X_val = X[lst_col][:position_validation_data].values, X[lst_col][position_validation_data:].values\n",
    "\n",
    "# Construction des Labels pour l'apprentissage et la validation.  Hot Encoding \n",
    "y_train, y_val = np.transpose([1-y[:position_validation_data], y[:position_validation_data]]), \\\n",
    "                  np.transpose([1-y[position_validation_data:], y[position_validation_data:]]) \n",
    "\n",
    "\n",
    "#Construction du modèle en appelant la fonction set_model\n",
    "model = set_model(init_learning_rate,dropout_prob) \n",
    "#définition d'une fonction History pour récupérer la fonction de coût et la métrique à chaque epoch.\n",
    "hist = History()\n",
    "model.fit(X_train, y_train, epochs=check_epochs, batch_size=128,validation_data=(X_val, y_val),verbose=False, callbacks=[hist])\n",
    "\n",
    "print(hist.history.keys())\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (40,20)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(hist.history['val_loss'], color= 'g')\n",
    "ax2.plot(hist.history['loss'], color= 'b')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('Validation data Error', color='g')\n",
    "ax2.set_ylabel('Training Data Error', color='b')\n",
    "plt.show()\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de cross validation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour un modèle  donné, on exécute la cross validation en utilisant un objet sss sklearn StratifiedShuffleSplit\n",
    "def cv_run(model, name, sss):  \n",
    "    loop=1\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "### A vous de completer les 2 lignes ci-dessous.\n",
    "### Il faut extraire les données d'apprentissage et de test des données du dataframe train en utilisant les index renvoyé par la fonction split\n",
    "### Vous pouvez vous inspirer du code du bloc \"Vérification du Sur-Apprentissage\"\n",
    "        # X_train, X_val = \n",
    "        # y_train, y_val = \n",
    "\n",
    "# Apprentissage et évaluation        \n",
    "        hist = History()\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=32,validation_data=(X_val, y_val),verbose=False, callbacks=[hist])\n",
    "        [loss, acc] = model.evaluate(X_val, y_val, batch_size=32,verbose=False)    \n",
    "\n",
    "#Ajout de la performance dans les dictionnaires \"loss_dict\" et \"acc_dict\"\n",
    "        if name in acc_dict:\n",
    "          acc_dict[name] += acc\n",
    "          loss_dict[name] += loss\n",
    "        else:\n",
    "          acc_dict[name] = acc\n",
    "          loss_dict[name] = loss\n",
    "#Affichage de l'avancement\n",
    "        print(loop,':',[loss, acc])\n",
    "        loop+=1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23b55b45-572b-7276-32e7-8f7a0dcfd25e",
    "_uuid": "4caf4fa8b262c029a4f220883b4c95ed3f25c88f"
   },
   "source": [
    "## Hyperparametrage ##\n",
    "### Ce traitement va être long. Commencer par une faible valeur du paramètre epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "31ded30a-8de4-6507-e7f7-5805a0f1eaf1",
    "_uuid": "b745532338e187d58ff5ee6d961d384b2a5f7bf9"
   },
   "outputs": [],
   "source": [
    "#Données utilisées pour la méthode split de l'objet StratifiedShuffleSplit\n",
    "X = train.values[0::, 1::]\n",
    "y = train.values[0::, 0]\n",
    "\n",
    "#Créatio d'un dictionnaire pour stocker les modèles\n",
    "model_dict={}\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.1, random_state=0)\n",
    "\n",
    "#Créatio d'un dataframe pour logger les résultatsc\n",
    "log_cols = [\"Classifier\", \"Accuracy\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "#Boucle sur des valeurs de init_learning_rate et de dropout_prob\n",
    "for init_learning_rate in lst_init_learning_rate:\n",
    "    for dropout_prob in  lst_dropout_prob :\n",
    "        #Initialisation des dictionnaires utilisés dans la cross validation \n",
    "        acc_dict = {}\n",
    "        loss_dict = {}\n",
    "        #Construction du nom du modèle, en fonction des paramètres\n",
    "        name=\"lr_%s_do_%s\"%(init_learning_rate,dropout_prob)\n",
    "        #Création de l'objet modèle\n",
    "        model = set_model(init_learning_rate,dropout_prob) \n",
    "        #Ajout du modèle au dico pour sélectionner le meilleur dans le suivant\n",
    "        model_dict[name]=model\n",
    "        cv_run(model, name, sss)        \n",
    "        # Calcul de la performance du modèle comme moyenne pour chaque itération dans cross-validation  \n",
    "        for clf in acc_dict:\n",
    "            acc_dict[clf] = acc_dict[clf] / n_splits\n",
    "            log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
    "            log = log.append(log_entry)\n",
    "print (log.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "438585cf-b7ad-73ba-49aa-87688ff21233",
    "_uuid": "f9e6b51b6b3c4cf3098bbdf90f984f827c2f7fd1"
   },
   "source": [
    "# Prediction #\n",
    "Maintenant on utilise le meilleur jeu de paramètre pour faire la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###A vous de completer les 3 lignes ci-dessous, sans oublier la normalisation !\n",
    "### Analyser les résultats du bloc précédent pour choisir le meilleur paramètre\n",
    "# best_model = model_dict[ ??? ]\n",
    "# X = ???\n",
    "# y = ???\n",
    "\n",
    "y_hot = np.transpose([1-y, y])\n",
    "\n",
    "#Apprentissage sur toutes les données, avec le modèle sélectionné\n",
    "best_model.fit(X,y_hot, epochs=epochs, batch_size=32,verbose=False)\n",
    "print(pd.DataFrame(best_model.evaluate(X, y_hot, batch_size=32,verbose=False),index=model.metrics_names))\n",
    "\n",
    "#Inférence des données du fichier test et Construction du fichier à envoyer à Kaggle \n",
    "prediction=best_model.predict(test.values, batch_size=32)\n",
    "results=pd.DataFrame(np.argmax(prediction,axis=1), index = finalfile_index, columns=['Survived'])\n",
    "results.to_csv('resultats.csv')\n",
    "print(results.sum())\n",
    "results.describe()"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
